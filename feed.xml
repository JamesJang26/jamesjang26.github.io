<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jamesjang26.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jamesjang26.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-04T15:20:26+00:00</updated><id>https://jamesjang26.github.io/feed.xml</id><title type="html">blank</title><subtitle>Master&apos;s candidate studying NLP @SNU MedINFO | Prev @YaleNLP. Research interests: AI for Science and Medicine, RAG, Reasoning, Agents, and Multimodal AI. </subtitle><entry><title type="html">[Project] Tennis Swing Analysis</title><link href="https://jamesjang26.github.io/blog/2024/tennis-swing-analysis/" rel="alternate" type="text/html" title="[Project] Tennis Swing Analysis"/><published>2024-11-04T00:00:00+00:00</published><updated>2024-11-04T00:00:00+00:00</updated><id>https://jamesjang26.github.io/blog/2024/tennis-swing-analysis</id><content type="html" xml:base="https://jamesjang26.github.io/blog/2024/tennis-swing-analysis/"><![CDATA[<h1 id="tennis-motion-analysis-system">Tennis Motion Analysis System</h1> <p>This project is a personal hobby endeavor aimed at creating a system that allows users to upload their own tennis videos, which are then analyzed to distinguish between <strong>forehand</strong> and <strong>backhand</strong> motions. The system also provides accuracy feedback by comparing the user’s form to a standard reference and offers suggestions for improvement.</p> <p>To kickstart this project, I used practice videos of one of my favorite tennis players, <strong>Carlos Alcaraz</strong>. Using these videos, I extracted joint positions, manually labeled the motions, and trained a model to classify each movement.</p> <hr/> <h2 id="project-overview">Project Overview</h2> <p>Tennis is a sport where recognizing and analyzing complex motions from various angles and speeds is crucial. Identifying and improving specific motions, like forehands and backhands, can offer valuable insights not only for professional players but also for amateurs looking to refine their techniques. The primary goal of this project is to develop a basic system that leverages <strong>LSTM (Long Short-Term Memory)</strong> networks to learn and analyze motion sequences in real-time or post-playback. In future stages, I plan to experiment with different models to enhance accuracy and select the most effective one for motion improvement.</p> <p>For this initial stage, I used <a href="https://github.com/google-ai-edge/mediapipe"><strong>MediaPipe</strong></a>, a pose estimation model, to extract joint positions from the videos. Using these extracted coordinates, I trained an LSTM model to classify motions, focusing on building a foundational system to distinguish tennis strokes.</p> <hr/> <h2 id="data-preparation">Data Preparation</h2> <h3 id="1-collecting-videos-from-youtube">1. Collecting Videos from YouTube</h3> <p>First, I collected practice videos of Carlos Alcaraz from <a href="https://www.youtube.com/@slowmotennis/videos">YouTube</a>(Thanks to @Slow-Mo Tennis), slicing into <strong>3 videos, each about 10 minutes long</strong>. Each video includes both <strong>forehand</strong> and <strong>backhand</strong> strokes, making them suitable for training and testing. I used <strong>PyTube</strong> to download these videos directly from YouTube.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pytube</span> <span class="kn">import</span> <span class="n">YouTube</span>

<span class="k">def</span> <span class="nf">download_video</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">save_path</span><span class="p">):</span>
    <span class="n">yt</span> <span class="o">=</span> <span class="nc">YouTube</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="n">yt</span><span class="p">.</span><span class="n">streams</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="n">progressive</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">file_extension</span><span class="o">=</span><span class="sh">'</span><span class="s">mp4</span><span class="sh">'</span><span class="p">).</span><span class="nf">first</span><span class="p">()</span>
    <span class="n">stream</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="n">output_path</span><span class="o">=</span><span class="n">save_path</span><span class="p">)</span>

<span class="nf">download_video</span><span class="p">(</span><span class="sh">"</span><span class="s">https://youtu.be/VIDEO_ID</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">./tennis_videos</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="2-extracting-poses-with-mediapipe">2. Extracting Poses with MediaPipe</h3> <p>Next, I used <strong>MediaPipe’s pose estimation model</strong> to process each video, extracting x, y, and z coordinates for key joints in each frame. MediaPipe was selected because of its straightforward setup and ability to quickly capture essential joint information, making it ideal for this project’s initial stages.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">mediapipe</span> <span class="k">as</span> <span class="n">mp</span>

<span class="n">mp_pose</span> <span class="o">=</span> <span class="n">mp</span><span class="p">.</span><span class="n">solutions</span><span class="p">.</span><span class="n">pose</span>
<span class="n">pose</span> <span class="o">=</span> <span class="n">mp_pose</span><span class="p">.</span><span class="nc">Pose</span><span class="p">(</span><span class="n">static_image_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">min_detection_confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">min_tracking_confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">VideoCapture</span><span class="p">(</span><span class="sh">'</span><span class="s">./tennis_videos/alcaraz_practice.mp4</span><span class="sh">'</span><span class="p">)</span>
<span class="n">landmarks_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">while</span> <span class="n">cap</span><span class="p">.</span><span class="nf">isOpened</span><span class="p">():</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">image_rgb</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">pose</span><span class="p">.</span><span class="nf">process</span><span class="p">(</span><span class="n">image_rgb</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">results</span><span class="p">.</span><span class="n">pose_landmarks</span><span class="p">:</span>
        <span class="n">frame_landmarks</span> <span class="o">=</span> <span class="p">[{</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">:</span> <span class="n">lm</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">:</span> <span class="n">lm</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="sh">"</span><span class="s">z</span><span class="sh">"</span><span class="p">:</span> <span class="n">lm</span><span class="p">.</span><span class="n">z</span><span class="p">}</span> <span class="k">for</span> <span class="n">lm</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="n">pose_landmarks</span><span class="p">.</span><span class="n">landmark</span><span class="p">]</span>
        <span class="n">landmarks_data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">frame_landmarks</span><span class="p">)</span>
</code></pre></div></div> <h3 id="3-manual-labeling-of-forehand-and-backhand-segments">3. Manual Labeling of Forehand and Backhand Segments</h3> <p>After extracting the joint coordinates, I used the <a href="https://github.com/cvat-ai/cvat"><strong>CVAT annotation tool</strong></a> to manually label each segment as either <strong>forehand</strong> or <strong>backhand</strong>. This labeled data was then organized into a dataset suitable for training the classification model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tennis_cvat1-480.webp 480w,/assets/img/tennis_cvat1-800.webp 800w,/assets/img/tennis_cvat1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/tennis_cvat1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 400px; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">CVAT annotation tool - Task list</figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tennis_cvat2-480.webp 480w,/assets/img/tennis_cvat2-800.webp 800w,/assets/img/tennis_cvat2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/tennis_cvat2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 400px; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">CVAT annotation tool - Labeling interface</figcaption> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of finalized annotation dataset
</span><span class="p">[</span>
    <span class="p">[</span>
        <span class="p">{</span>
            <span class="sh">"</span><span class="s">frame_id</span><span class="sh">"</span><span class="p">:</span> <span class="mi">170</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Backhand</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">landmarks</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.45166587829589844</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5477275848388672</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">z</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.09219522774219513</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">visibility</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.9998981952667236</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.44997501373291016</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5417541265487671</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">z</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.07722048461437225</span><span class="p">,</span>
                    <span class="sh">"</span><span class="s">visibility</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.9999018907546997</span>
                <span class="p">},</span>
                <span class="c1"># ommited below
</span>            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">]</span>
</code></pre></div></div> <hr/> <h2 id="model-training">Model Training</h2> <h3 id="data-preprocessing">Data Preprocessing</h3> <p>To prepare the data for training, I standardized and padded each sequence so that they were compatible with the model’s input requirements. Using <code class="language-plaintext highlighter-rouge">StandardScaler</code>, I normalized each joint coordinate, and padded shorter sequences to ensure consistent input length across all sequences.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="n">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="kn">import</span> <span class="n">joblib</span>

<span class="c1"># Mount Google Drive
</span><span class="n">drive</span><span class="p">.</span><span class="nf">mount</span><span class="p">(</span><span class="sh">'</span><span class="s">/content/drive</span><span class="sh">'</span><span class="p">,</span> <span class="n">force_remount</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Define paths to multiple JSON files for each video and annotation
</span><span class="n">json_paths</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">/content/drive/My Drive/tennis/[1]combined_sequences.json</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">/content/drive/My Drive/tennis/[2]combined_sequences.json</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">/content/drive/My Drive/tennis/[3]combined_sequences.json</span><span class="sh">'</span>
<span class="p">]</span>

<span class="c1"># Load and combine data
</span><span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">label_map</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">Forehand</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">Backhand</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>  <span class="c1"># Map labels to numeric values
</span>
<span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">json_paths</span><span class="p">:</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">sequence_landmarks</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">frame_data</span> <span class="ow">in</span> <span class="n">sequence</span><span class="p">:</span>
                <span class="n">landmarks</span> <span class="o">=</span> <span class="n">frame_data</span><span class="p">[</span><span class="sh">'</span><span class="s">landmarks</span><span class="sh">'</span><span class="p">]</span>
                <span class="c1"># Check if landmarks is a list or dict and flatten accordingly
</span>                <span class="n">landmarks_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">coord</span> <span class="k">for</span> <span class="n">lm</span> <span class="ow">in</span> <span class="n">landmarks</span> <span class="k">for</span> <span class="n">coord</span> <span class="ow">in</span> <span class="p">(</span><span class="n">lm</span><span class="p">[</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">],</span> <span class="n">lm</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">],</span> <span class="n">lm</span><span class="p">[</span><span class="sh">'</span><span class="s">z</span><span class="sh">'</span><span class="p">])]</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">landmarks</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="p">[</span><span class="n">value</span> <span class="k">for</span> <span class="n">lm</span> <span class="ow">in</span> <span class="n">landmarks</span><span class="p">.</span><span class="nf">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="p">(</span><span class="n">lm</span><span class="p">[</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">],</span> <span class="n">lm</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">],</span> <span class="n">lm</span><span class="p">[</span><span class="sh">'</span><span class="s">z</span><span class="sh">'</span><span class="p">])]</span>
                <span class="n">sequence_landmarks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">landmarks_flat</span><span class="p">)</span>
            <span class="n">X</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">sequence_landmarks</span><span class="p">)</span>
            <span class="n">y</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">label_map</span><span class="p">[</span><span class="n">sequence</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]])</span>

<span class="c1"># Calculate the maximum sequence length for padding
</span><span class="n">max_seq_length</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span>

<span class="c1"># Pad sequences to ensure they are of the same length
</span><span class="n">X_padded</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">seq</span> <span class="o">+</span> <span class="p">[[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_seq_length</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span> <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">X</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Normalize data with StandardScaler and reshape back
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X_padded</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_padded</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_padded</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X_padded</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Save scaler for future use
</span><span class="n">scaler_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/content/drive/My Drive/tennis/scaler.pkl</span><span class="sh">"</span>
<span class="n">joblib</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">scaler</span><span class="p">,</span> <span class="n">scaler_path</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">StandardScaler saved at </span><span class="sh">'</span><span class="si">{</span><span class="n">scaler_path</span><span class="si">}</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Split data into train and test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X_padded</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Convert to torch tensors
</span><span class="n">X_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">X_test_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
<span class="n">y_test_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
</code></pre></div></div> <h3 id="model-training-code">Model Training Code</h3> <p>I implemented an LSTM model, structured to analyze sequences of poses and classify each segment as either a forehand or a backhand based on the final frame in each sequence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="k">class</span> <span class="nc">LSTMModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">LSTMModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">hidden_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">hidden_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Model initialization
</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">X_train_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">LSTMModel</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Training setup
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train the model
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="nc">TennisDataset</span><span class="p">(</span><span class="n">X_train_tensor</span><span class="p">,</span> <span class="n">y_train_tensor</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="n">X_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s">], Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Save the trained model
</span><span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="sh">"</span><span class="s">/content/drive/My Drive/tennis/tennis_model.pth</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="testing-and-analysis-of-results">Testing and Analysis of Results</h2> <h3 id="visualization-of-predictions-on-test-videos">Visualization of Predictions on Test Videos</h3> <p>Using the trained model, I analyzed a test video by inputting each frame’s joint positions and obtaining a forehand or backhand prediction for each segment. The predictions were visually represented on the video, allowing for straightforward evaluation of the model’s performance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">mediapipe</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">import</span> <span class="n">joblib</span>

<span class="c1"># Load the trained model and scaler
</span><span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">/content/drive/My Drive/tennis/tennis_model.pth</span><span class="sh">"</span><span class="p">))</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">joblib</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">/content/drive/My Drive/tennis/scaler.pkl</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Set up MediaPipe and video paths
</span><span class="n">mp_pose</span> <span class="o">=</span> <span class="n">mp</span><span class="p">.</span><span class="n">solutions</span><span class="p">.</span><span class="n">pose</span>
<span class="n">video_path</span> <span class="o">=</span> <span class="sh">'</span><span class="s">/content/drive/My Drive/tennis/[3]pose_output.mp4</span><span class="sh">'</span>
<span class="n">output_video_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/content/drive/My Drive/tennis/[3]annotated_video.mp4</span><span class="sh">"</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">VideoCapture</span><span class="p">(</span><span class="n">video_path</span><span class="p">)</span>
<span class="n">fourcc</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">VideoWriter_fourcc</span><span class="p">(</span><span class="o">*</span><span class="sh">'</span><span class="s">mp4v</span><span class="sh">'</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">VideoWriter</span><span class="p">(</span><span class="n">output_video_path</span><span class="p">,</span> <span class="n">fourcc</span><span class="p">,</span> <span class="n">cap</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">CAP_PROP_FPS</span><span class="p">),</span> 
                      <span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">cap</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">CAP_PROP_FRAME_WIDTH</span><span class="p">)),</span> <span class="nf">int</span><span class="p">(</span><span class="n">cap</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">CAP_PROP_FRAME_HEIGHT</span><span class="p">))))</span>

<span class="c1"># Annotate video with predictions
</span><span class="k">with</span> <span class="n">mp_pose</span><span class="p">.</span><span class="nc">Pose</span><span class="p">(</span><span class="n">static_image_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">min_detection_confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">min_tracking_confidence</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="k">as</span> <span class="n">pose</span><span class="p">:</span>
    <span class="k">while</span> <span class="n">cap</span><span class="p">.</span><span class="nf">isOpened</span><span class="p">():</span>
        <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># Extract landmarks
</span>        <span class="n">image_rgb</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">pose</span><span class="p">.</span><span class="nf">process</span><span class="p">(</span><span class="n">image_rgb</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">results</span><span class="p">.</span><span class="n">pose_landmarks</span><span class="p">:</span>
            <span class="n">landmarks_flat</span> <span class="o">=</span> <span class="p">[</span><span class="n">coord</span> <span class="k">for</span> <span class="n">lm</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="n">pose_landmarks</span><span class="p">.</span><span class="n">landmark</span> <span class="k">for</span> <span class="n">coord</span> <span class="ow">in</span> <span class="p">(</span><span class="n">lm</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">lm</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">lm</span><span class="p">.</span><span class="n">z</span><span class="p">)]</span>
            <span class="n">landmarks_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">landmarks_flat</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">landmarks_norm</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">landmarks_array</span><span class="p">)</span>
            <span class="n">landmarks_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">landmarks_norm</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Predict using the model
</span>            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
                <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">landmarks_tensor</span><span class="p">)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Forehand</span><span class="sh">"</span> <span class="k">if</span> <span class="n">predicted</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="sh">"</span><span class="s">Backhand</span><span class="sh">"</span>

            <span class="c1"># Annotate frame
</span>            <span class="n">cv2</span><span class="p">.</span><span class="nf">putText</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">cv2</span><span class="p">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="sh">"</span><span class="s">Forehand</span><span class="sh">"</span> <span class="nf">else </span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">color</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="sh">"</span><span class="s">Forehand</span><span class="sh">"</span> <span class="nf">else </span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">)</span>
            <span class="n">cv2</span><span class="p">.</span><span class="nf">rectangle</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">frame</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">5</span><span class="p">),</span> <span class="n">color</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

        <span class="n">out</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

<span class="n">cap</span><span class="p">.</span><span class="nf">release</span><span class="p">()</span>
<span class="n">out</span><span class="p">.</span><span class="nf">release</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Annotated video saved to Google Drive:</span><span class="sh">"</span><span class="p">,</span> <span class="n">output_video_path</span><span class="p">)</span>
</code></pre></div></div> <h3 id="observed-issues-and-potential-improvements">Observed Issues and Potential Improvements</h3> <ol> <li><strong>Labeling Only the Joint Area</strong>: Currently, labels are applied to the entire frame, which reduces clarity. Focusing the labels on just the joint area would improve the visualization.</li> <li><strong>Smoothing Across Segments</strong>: Rather than predicting each frame independently, applying segment-based smoothing would improve stability by reducing prediction fluctuations across frames.</li> <li><strong>Limited Data Size</strong>: The small dataset size limits the model’s ability to generalize across different angles and conditions. Expanding the dataset or using data augmentation would likely enhance model performance.</li> </ol> <hr/> <h2 id="conclusion">Conclusion</h2> <p>In this project, I used practice videos of Carlos Alcaraz to train an LSTM model that distinguishes between forehand and backhand motions in tennis. By combining MediaPipe’s pose estimation capabilities with the LSTM model, I created a foundational system that analyzes joint coordinates to predict tennis strokes, with visual feedback provided on the test video.</p> <p>However, several limitations were identified: (1) the small dataset size, which limited learning, and (2) the model’s tendency to label every frame across the entire screen, rather than focusing on specific joint areas. Moreover, there’s a need to refine the output by adding smoothing to improve the stability of predictions over sequential frames.</p> <p>Going forward, I plan to improve this system by collecting more data, applying data augmentation techniques, and implementing segment-based smoothing. These enhancements will enable the creation of a more robust and precise tennis motion analysis system.</p>]]></content><author><name></name></author><category term="blog"/><category term="Project"/><category term="AI"/><category term="ComputerVision"/><summary type="html"><![CDATA[A system to analyze tennis swings using LSTM and MediaPipe for motion classification.]]></summary></entry></feed>